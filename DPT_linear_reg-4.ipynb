{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mgpoFoxIJWrl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgpoFoxIJWrl",
        "outputId": "63d4ca4f-c0d3-454b-9855-d180ca7f7cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce21e60-73f2-421d-b8d5-c8a7023b20a2",
      "metadata": {
        "id": "1ce21e60-73f2-421d-b8d5-c8a7023b20a2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Model, GPT2Config\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff786d5",
      "metadata": {
        "id": "bff786d5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"error\", RuntimeWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bdba263-b726-4316-80b5-2be5e2fb6c91",
      "metadata": {
        "id": "7bdba263-b726-4316-80b5-2be5e2fb6c91"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d945d95-155d-4bed-b879-5af6f0d32f21",
      "metadata": {
        "id": "5d945d95-155d-4bed-b879-5af6f0d32f21"
      },
      "outputs": [],
      "source": [
        "A = 10 # number of actions\n",
        "N = 40000 # number of offline samples\n",
        "d = 2\n",
        "n = 200\n",
        "eps = 1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10bf0278",
      "metadata": {
        "id": "10bf0278"
      },
      "outputs": [],
      "source": [
        "class GaussianBandit:\n",
        "    def __init__(self):\n",
        "        self.A = 10\n",
        "        self.variance = 0.3 ** 2\n",
        "        self.prior_mean = 0\n",
        "        self.prior_var = 1\n",
        "        self.means = np.ones(self.A) * self.prior_mean\n",
        "        self.variances = np.ones(self.A) * self.prior_var\n",
        "        self.counts = np.zeros(self.A)\n",
        "\n",
        "    def select_arm(self):\n",
        "        sampled_rewards = np.random.normal(self.means, np.sqrt(self.variances))\n",
        "        return np.argmax(sampled_rewards)\n",
        "\n",
        "    def update(self, chosen_arm, arm_rewards):\n",
        "        self.counts[chosen_arm] += 1\n",
        "        count = self.counts[chosen_arm]\n",
        "\n",
        "        arm_mean = np.mean(arm_rewards)\n",
        "        prior_weight = self.variance / (self.variance + (count * self.prior_var))\n",
        "        new_mean = prior_weight * self.prior_mean + (1 - prior_weight) * arm_mean\n",
        "        new_variance = 1 / (1 / self.prior_var + count / self.variance)\n",
        "        # try:\n",
        "        #     new_variance = 1 / (1 / self.prior_var + count / self.variance)\n",
        "        # except RuntimeWarning:\n",
        "        #     new_variance = 0\n",
        "\n",
        "        self.means[chosen_arm] = new_mean\n",
        "        self.variances[chosen_arm] = new_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzHXwg69vKO-",
      "metadata": {
        "id": "MzHXwg69vKO-"
      },
      "outputs": [],
      "source": [
        "phi = np.random.normal(0, np.sqrt(1/d), size=(A, d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3d2a83",
      "metadata": {
        "id": "de3d2a83"
      },
      "outputs": [],
      "source": [
        "def generate_in_context_dataset(n=200):\n",
        "    actions, rewards = [], []\n",
        "    theta = np.random.normal(0, np.sqrt(1/d), size=d)\n",
        "    mus = np.dot(phi, theta)\n",
        "    bandit = GaussianBandit()\n",
        "    arm_rewards_list = [[] for _ in range(A)]\n",
        "\n",
        "    for _ in range(n):\n",
        "        chosen_arm = bandit.select_arm()\n",
        "        reward = mus[chosen_arm] + np.random.normal(0, 0.3)#np.random.binomial(1, bandit.means[chosen_arm])\n",
        "        arm_rewards_list[chosen_arm].append(reward)\n",
        "        bandit.update(chosen_arm, arm_rewards_list[chosen_arm])\n",
        "        actions.append(chosen_arm)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    p = bandit.counts / n\n",
        "    return actions, rewards, mus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7d8d1c",
      "metadata": {
        "id": "de7d8d1c"
      },
      "outputs": [],
      "source": [
        "def generate_B(N=40000):\n",
        "    dsets, actions, rewards, coeffs = [], [], [], []\n",
        "\n",
        "    for _ in tqdm(range(N)):\n",
        "        a, r, mus = generate_in_context_dataset()\n",
        "        actions.append(np.array(a))\n",
        "        rewards.append(r)\n",
        "        # coeffs.append(np.exp(100 * mus[a] - np.dot(p, mus)))\n",
        "        coeffs.append(np.exp(mus[a] - np.array(r).mean()))\n",
        "        # mu_empirical = np.zeros(A)\n",
        "        # for i in range(A):\n",
        "        #     if np.argwhere(a==i).size > 0:\n",
        "        #         mu_empirical[i] = np.array(r[np.where(a==i)]).mean()\n",
        "        # coeffs.append(np.maximum(mu_empirical[a] - np.array(r).mean(), 0))\n",
        "\n",
        "        a_one_hot = np.zeros((n, A))\n",
        "        a_one_hot[np.arange(n), a] = 1\n",
        "\n",
        "        X = np.zeros((n, A + 3), np.float32)\n",
        "        X[:, 0] = 1\n",
        "        X[:, 1:A + 1] = a_one_hot\n",
        "        X[:, -2] = 1\n",
        "        X[:, -1] = r\n",
        "        dsets.append(X)\n",
        "\n",
        "    return dsets, actions, coeffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8224dd6d-b47a-4590-b45f-533496b9368f",
      "metadata": {
        "id": "8224dd6d-b47a-4590-b45f-533496b9368f"
      },
      "outputs": [],
      "source": [
        "class BanditDataset(Dataset):\n",
        "    def __init__(self, dsets, actions, coefficients):\n",
        "        self.dsets = dsets\n",
        "        self.actions = actions\n",
        "        self.coefficients = coefficients\n",
        "\n",
        "        self.first = np.zeros((1, A + 3), dtype=np.float32)\n",
        "        self.first[0, 0] = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dsets)\n",
        "\n",
        "    def __getitem__(self, idx): # no shuflle\n",
        "        sample_ds = self.dsets[idx]\n",
        "        sample_ds = np.concatenate((self.first, sample_ds))\n",
        "        sample_actions = self.actions[idx]\n",
        "        sample_coefficients = self.coefficients[idx]\n",
        "\n",
        "        return sample_ds, sample_actions, sample_coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de84ccd0-4156-4f20-9b53-1fc8be5f7ab8",
      "metadata": {
        "id": "de84ccd0-4156-4f20-9b53-1fc8be5f7ab8"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_states, n_positions=201, n_embd=32, n_layer=4, n_head=4):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        configuration = GPT2Config(\n",
        "            n_positions=n_positions,\n",
        "            n_embd=n_embd,\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "        )\n",
        "        self.name = f\"gpt2_embd={n_embd}_layer={n_layer}_head={n_head}\"\n",
        "\n",
        "        self.n_positions = n_positions\n",
        "        self.n_dims = n_states\n",
        "        self._read_in = nn.Linear(n_states + 3, n_embd)\n",
        "        self._backbone = GPT2Model(configuration)\n",
        "        self._read_out = nn.Linear(n_embd, 10)\n",
        "        self._flatten = nn.Flatten(0, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embeds = self._read_in(X)\n",
        "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
        "        logit = self._read_out(output)[:, :-1]\n",
        "        logit = self._flatten(logit)\n",
        "\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6c5d62-3b19-48cf-bc06-804d504b1ef2",
      "metadata": {
        "id": "6b6c5d62-3b19-48cf-bc06-804d504b1ef2"
      },
      "outputs": [],
      "source": [
        "def loss_fn(pred, a, c):\n",
        "    ce_loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    return torch.mean(ce_loss_fn(pred, a) * c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e4c60a-d79c-40ba-85e5-a733a4a36a5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0e4c60a-d79c-40ba-85e5-a733a4a36a5e",
        "outputId": "9d3eb395-0a31-467b-a53d-77b0e8a6f501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [05:28<00:00, 121.68it/s]\n",
            "100%|██████████| 10000/10000 [01:22<00:00, 120.66it/s]\n"
          ]
        }
      ],
      "source": [
        "dsets_train, actions_train, coefficients_train = generate_B(N=N)\n",
        "dsets_val, actions_val, coefficients_val = generate_B(N=N//4)\n",
        "\n",
        "data_train = BanditDataset(dsets_train, actions_train, coefficients_train)\n",
        "data_val = BanditDataset(dsets_val, actions_val, coefficients_val)\n",
        "\n",
        "# torch.save(data_train, 'train_data.pth')\n",
        "# torch.save(data_val, 'val_data.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gnBbReTnpNxn",
      "metadata": {
        "id": "gnBbReTnpNxn"
      },
      "outputs": [],
      "source": [
        "#data_train = torch.load('/content/train_data.pth')\n",
        "#data_val = torch.load('val_data.pth')\n",
        "train_dataloader = DataLoader(data_train, batch_size=768)\n",
        "val_dataloader = DataLoader(data_val, batch_size=768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74e74bb-0651-464e-8942-d5a50719c616",
      "metadata": {
        "id": "c74e74bb-0651-464e-8942-d5a50719c616"
      },
      "outputs": [],
      "source": [
        "model = TransformerModel(n_states=10)\n",
        "model.to(device)\n",
        "model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8289d6-2481-4a96-8ef6-7255d573d44f",
      "metadata": {
        "id": "bc8289d6-2481-4a96-8ef6-7255d573d44f"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, a, c) in enumerate(train_dataloader):\n",
        "        X = X.to(device)\n",
        "        pred = model(X)\n",
        "        a = a.flatten().to(device)\n",
        "        c = c.flatten().to(device)\n",
        "\n",
        "        loss = loss_fn(pred, a, c)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7873169-e185-4254-9248-99591294a847",
      "metadata": {
        "id": "d7873169-e185-4254-9248-99591294a847"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, a, c in dataloader:\n",
        "            X = X.to(device)\n",
        "            pred = model(X)\n",
        "            a = a.flatten().to(device)\n",
        "            c = c.flatten().to(device)\n",
        "\n",
        "            loss = loss_fn(pred, a, c)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= num_batches\n",
        "    print(f\"Val loss: {val_loss:>8f} \\n\")\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259356c4-b704-45b9-aab2-1d789009d5fd",
      "metadata": {
        "id": "259356c4-b704-45b9-aab2-1d789009d5fd"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f051de06-dde7-4825-b451-3cf9822fd774",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f051de06-dde7-4825-b451-3cf9822fd774",
        "outputId": "19437572-12ed-433e-9e82-ede0a77b461c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.566917  [  768/40000]\n",
            "loss: 1.586003  [ 8448/40000]\n",
            "loss: 1.028736  [16128/40000]\n",
            "loss: 0.871758  [23808/40000]\n",
            "loss: 0.787190  [31488/40000]\n",
            "loss: 0.712027  [39168/40000]\n",
            "Val loss: 0.704562 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.736847  [  768/40000]\n",
            "loss: 0.696997  [ 8448/40000]\n",
            "loss: 0.668082  [16128/40000]\n",
            "loss: 0.622341  [23808/40000]\n",
            "loss: 0.616972  [31488/40000]\n",
            "loss: 0.594811  [39168/40000]\n",
            "Val loss: 0.592747 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.621369  [  768/40000]\n",
            "loss: 0.603644  [ 8448/40000]\n",
            "loss: 0.599267  [16128/40000]\n",
            "loss: 0.564134  [23808/40000]\n",
            "loss: 0.569910  [31488/40000]\n",
            "loss: 0.556348  [39168/40000]\n",
            "Val loss: 0.556708 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.582966  [  768/40000]\n",
            "loss: 0.571447  [ 8448/40000]\n",
            "loss: 0.572472  [16128/40000]\n",
            "loss: 0.541806  [23808/40000]\n",
            "loss: 0.552593  [31488/40000]\n",
            "loss: 0.540587  [39168/40000]\n",
            "Val loss: 0.541616 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.565768  [  768/40000]\n",
            "loss: 0.558413  [ 8448/40000]\n",
            "loss: 0.558580  [16128/40000]\n",
            "loss: 0.529693  [23808/40000]\n",
            "loss: 0.542097  [31488/40000]\n",
            "loss: 0.531631  [39168/40000]\n",
            "Val loss: 0.533083 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.557018  [  768/40000]\n",
            "loss: 0.550520  [ 8448/40000]\n",
            "loss: 0.551279  [16128/40000]\n",
            "loss: 0.523158  [23808/40000]\n",
            "loss: 0.537842  [31488/40000]\n",
            "loss: 0.526122  [39168/40000]\n",
            "Val loss: 0.528039 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.553440  [  768/40000]\n",
            "loss: 0.545285  [ 8448/40000]\n",
            "loss: 0.546455  [16128/40000]\n",
            "loss: 0.518887  [23808/40000]\n",
            "loss: 0.533502  [31488/40000]\n",
            "loss: 0.523071  [39168/40000]\n",
            "Val loss: 0.524097 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.548012  [  768/40000]\n",
            "loss: 0.541105  [ 8448/40000]\n",
            "loss: 0.542200  [16128/40000]\n",
            "loss: 0.515090  [23808/40000]\n",
            "loss: 0.530843  [31488/40000]\n",
            "loss: 0.519998  [39168/40000]\n",
            "Val loss: 0.521068 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.544417  [  768/40000]\n",
            "loss: 0.537975  [ 8448/40000]\n",
            "loss: 0.539461  [16128/40000]\n",
            "loss: 0.511591  [23808/40000]\n",
            "loss: 0.528402  [31488/40000]\n",
            "loss: 0.517194  [39168/40000]\n",
            "Val loss: 0.518412 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.541385  [  768/40000]\n",
            "loss: 0.535968  [ 8448/40000]\n",
            "loss: 0.537192  [16128/40000]\n",
            "loss: 0.510211  [23808/40000]\n",
            "loss: 0.526053  [31488/40000]\n",
            "loss: 0.515380  [39168/40000]\n",
            "Val loss: 0.517378 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.540249  [  768/40000]\n",
            "loss: 0.534203  [ 8448/40000]\n",
            "loss: 0.535401  [16128/40000]\n",
            "loss: 0.508758  [23808/40000]\n",
            "loss: 0.525537  [31488/40000]\n",
            "loss: 0.514184  [39168/40000]\n",
            "Val loss: 0.515544 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.537944  [  768/40000]\n",
            "loss: 0.532580  [ 8448/40000]\n",
            "loss: 0.533711  [16128/40000]\n",
            "loss: 0.506805  [23808/40000]\n",
            "loss: 0.523082  [31488/40000]\n",
            "loss: 0.512537  [39168/40000]\n",
            "Val loss: 0.514207 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.536418  [  768/40000]\n",
            "loss: 0.530912  [ 8448/40000]\n",
            "loss: 0.532018  [16128/40000]\n",
            "loss: 0.505847  [23808/40000]\n",
            "loss: 0.521790  [31488/40000]\n",
            "loss: 0.511947  [39168/40000]\n",
            "Val loss: 0.513226 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.534981  [  768/40000]\n",
            "loss: 0.529618  [ 8448/40000]\n",
            "loss: 0.530652  [16128/40000]\n",
            "loss: 0.504856  [23808/40000]\n",
            "loss: 0.520735  [31488/40000]\n",
            "loss: 0.510515  [39168/40000]\n",
            "Val loss: 0.512162 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.533697  [  768/40000]\n",
            "loss: 0.529150  [ 8448/40000]\n",
            "loss: 0.529195  [16128/40000]\n",
            "loss: 0.503640  [23808/40000]\n",
            "loss: 0.519998  [31488/40000]\n",
            "loss: 0.509260  [39168/40000]\n",
            "Val loss: 0.511440 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.533152  [  768/40000]\n",
            "loss: 0.527509  [ 8448/40000]\n",
            "loss: 0.528054  [16128/40000]\n",
            "loss: 0.502481  [23808/40000]\n",
            "loss: 0.518278  [31488/40000]\n",
            "loss: 0.508702  [39168/40000]\n",
            "Val loss: 0.510152 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.531189  [  768/40000]\n",
            "loss: 0.526653  [ 8448/40000]\n",
            "loss: 0.526775  [16128/40000]\n",
            "loss: 0.501545  [23808/40000]\n",
            "loss: 0.517462  [31488/40000]\n",
            "loss: 0.507705  [39168/40000]\n",
            "Val loss: 0.508971 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.530684  [  768/40000]\n",
            "loss: 0.525976  [ 8448/40000]\n",
            "loss: 0.525629  [16128/40000]\n",
            "loss: 0.500807  [23808/40000]\n",
            "loss: 0.515664  [31488/40000]\n",
            "loss: 0.506434  [39168/40000]\n",
            "Val loss: 0.508103 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.529454  [  768/40000]\n",
            "loss: 0.524842  [ 8448/40000]\n",
            "loss: 0.524551  [16128/40000]\n",
            "loss: 0.500166  [23808/40000]\n",
            "loss: 0.515135  [31488/40000]\n",
            "loss: 0.505458  [39168/40000]\n",
            "Val loss: 0.507276 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.528668  [  768/40000]\n",
            "loss: 0.524017  [ 8448/40000]\n",
            "loss: 0.522795  [16128/40000]\n",
            "loss: 0.499387  [23808/40000]\n",
            "loss: 0.513853  [31488/40000]\n",
            "loss: 0.505119  [39168/40000]\n",
            "Val loss: 0.506625 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.527658  [  768/40000]\n",
            "loss: 0.523839  [ 8448/40000]\n",
            "loss: 0.522491  [16128/40000]\n",
            "loss: 0.498023  [23808/40000]\n",
            "loss: 0.513245  [31488/40000]\n",
            "loss: 0.504332  [39168/40000]\n",
            "Val loss: 0.506094 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.526794  [  768/40000]\n",
            "loss: 0.522517  [ 8448/40000]\n",
            "loss: 0.522192  [16128/40000]\n",
            "loss: 0.496986  [23808/40000]\n",
            "loss: 0.512702  [31488/40000]\n",
            "loss: 0.503613  [39168/40000]\n",
            "Val loss: 0.505267 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.525791  [  768/40000]\n",
            "loss: 0.522364  [ 8448/40000]\n",
            "loss: 0.520468  [16128/40000]\n",
            "loss: 0.496749  [23808/40000]\n",
            "loss: 0.512683  [31488/40000]\n",
            "loss: 0.502983  [39168/40000]\n",
            "Val loss: 0.504685 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.525185  [  768/40000]\n",
            "loss: 0.521863  [ 8448/40000]\n",
            "loss: 0.520348  [16128/40000]\n",
            "loss: 0.495825  [23808/40000]\n",
            "loss: 0.512046  [31488/40000]\n",
            "loss: 0.502187  [39168/40000]\n",
            "Val loss: 0.504096 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.524886  [  768/40000]\n",
            "loss: 0.521398  [ 8448/40000]\n",
            "loss: 0.519038  [16128/40000]\n",
            "loss: 0.495562  [23808/40000]\n",
            "loss: 0.511284  [31488/40000]\n",
            "loss: 0.502345  [39168/40000]\n",
            "Val loss: 0.504078 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.524956  [  768/40000]\n",
            "loss: 0.521029  [ 8448/40000]\n",
            "loss: 0.518900  [16128/40000]\n",
            "loss: 0.494930  [23808/40000]\n",
            "loss: 0.511050  [31488/40000]\n",
            "loss: 0.501795  [39168/40000]\n",
            "Val loss: 0.503851 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.524210  [  768/40000]\n",
            "loss: 0.520949  [ 8448/40000]\n",
            "loss: 0.518674  [16128/40000]\n",
            "loss: 0.494854  [23808/40000]\n",
            "loss: 0.510562  [31488/40000]\n",
            "loss: 0.501476  [39168/40000]\n",
            "Val loss: 0.503130 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.523831  [  768/40000]\n",
            "loss: 0.520286  [ 8448/40000]\n",
            "loss: 0.518339  [16128/40000]\n",
            "loss: 0.494094  [23808/40000]\n",
            "loss: 0.510275  [31488/40000]\n",
            "loss: 0.501234  [39168/40000]\n",
            "Val loss: 0.502998 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.523151  [  768/40000]\n",
            "loss: 0.519279  [ 8448/40000]\n",
            "loss: 0.518034  [16128/40000]\n",
            "loss: 0.493437  [23808/40000]\n",
            "loss: 0.509295  [31488/40000]\n",
            "loss: 0.500738  [39168/40000]\n",
            "Val loss: 0.502531 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.522520  [  768/40000]\n",
            "loss: 0.519412  [ 8448/40000]\n",
            "loss: 0.517375  [16128/40000]\n",
            "loss: 0.493121  [23808/40000]\n",
            "loss: 0.509325  [31488/40000]\n",
            "loss: 0.500278  [39168/40000]\n",
            "Val loss: 0.502309 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.522283  [  768/40000]\n",
            "loss: 0.518756  [ 8448/40000]\n",
            "loss: 0.516693  [16128/40000]\n",
            "loss: 0.492988  [23808/40000]\n",
            "loss: 0.508894  [31488/40000]\n",
            "loss: 0.500313  [39168/40000]\n",
            "Val loss: 0.501939 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.522014  [  768/40000]\n",
            "loss: 0.519031  [ 8448/40000]\n",
            "loss: 0.516316  [16128/40000]\n",
            "loss: 0.492537  [23808/40000]\n",
            "loss: 0.508613  [31488/40000]\n",
            "loss: 0.499749  [39168/40000]\n",
            "Val loss: 0.501517 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.521589  [  768/40000]\n",
            "loss: 0.517726  [ 8448/40000]\n",
            "loss: 0.516620  [16128/40000]\n",
            "loss: 0.491987  [23808/40000]\n",
            "loss: 0.507863  [31488/40000]\n",
            "loss: 0.499265  [39168/40000]\n",
            "Val loss: 0.501240 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.521328  [  768/40000]\n",
            "loss: 0.517624  [ 8448/40000]\n",
            "loss: 0.515577  [16128/40000]\n",
            "loss: 0.491864  [23808/40000]\n",
            "loss: 0.507329  [31488/40000]\n",
            "loss: 0.498436  [39168/40000]\n",
            "Val loss: 0.500700 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.520709  [  768/40000]\n",
            "loss: 0.516871  [ 8448/40000]\n",
            "loss: 0.515329  [16128/40000]\n",
            "loss: 0.491215  [23808/40000]\n",
            "loss: 0.507075  [31488/40000]\n",
            "loss: 0.497920  [39168/40000]\n",
            "Val loss: 0.500216 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.520634  [  768/40000]\n",
            "loss: 0.517341  [ 8448/40000]\n",
            "loss: 0.514869  [16128/40000]\n",
            "loss: 0.490303  [23808/40000]\n",
            "loss: 0.506415  [31488/40000]\n",
            "loss: 0.497476  [39168/40000]\n",
            "Val loss: 0.499419 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.519977  [  768/40000]\n",
            "loss: 0.516227  [ 8448/40000]\n",
            "loss: 0.513973  [16128/40000]\n",
            "loss: 0.490166  [23808/40000]\n",
            "loss: 0.505482  [31488/40000]\n",
            "loss: 0.496432  [39168/40000]\n",
            "Val loss: 0.499023 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.519843  [  768/40000]\n",
            "loss: 0.515262  [ 8448/40000]\n",
            "loss: 0.512550  [16128/40000]\n",
            "loss: 0.489232  [23808/40000]\n",
            "loss: 0.505377  [31488/40000]\n",
            "loss: 0.496083  [39168/40000]\n",
            "Val loss: 0.497150 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.518306  [  768/40000]\n",
            "loss: 0.515297  [ 8448/40000]\n",
            "loss: 0.512532  [16128/40000]\n",
            "loss: 0.488526  [23808/40000]\n",
            "loss: 0.504752  [31488/40000]\n",
            "loss: 0.495089  [39168/40000]\n",
            "Val loss: 0.496467 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.517630  [  768/40000]\n",
            "loss: 0.514164  [ 8448/40000]\n",
            "loss: 0.511611  [16128/40000]\n",
            "loss: 0.488389  [23808/40000]\n",
            "loss: 0.503494  [31488/40000]\n",
            "loss: 0.495148  [39168/40000]\n",
            "Val loss: 0.496353 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.517391  [  768/40000]\n",
            "loss: 0.513343  [ 8448/40000]\n",
            "loss: 0.511066  [16128/40000]\n",
            "loss: 0.487370  [23808/40000]\n",
            "loss: 0.502855  [31488/40000]\n",
            "loss: 0.494170  [39168/40000]\n",
            "Val loss: 0.495430 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.516734  [  768/40000]\n",
            "loss: 0.513786  [ 8448/40000]\n",
            "loss: 0.510449  [16128/40000]\n",
            "loss: 0.487211  [23808/40000]\n",
            "loss: 0.502772  [31488/40000]\n",
            "loss: 0.494020  [39168/40000]\n",
            "Val loss: 0.494591 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.515795  [  768/40000]\n",
            "loss: 0.512781  [ 8448/40000]\n",
            "loss: 0.510831  [16128/40000]\n",
            "loss: 0.486459  [23808/40000]\n",
            "loss: 0.502624  [31488/40000]\n",
            "loss: 0.493755  [39168/40000]\n",
            "Val loss: 0.494438 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.515597  [  768/40000]\n",
            "loss: 0.511278  [ 8448/40000]\n",
            "loss: 0.510167  [16128/40000]\n",
            "loss: 0.485923  [23808/40000]\n",
            "loss: 0.502081  [31488/40000]\n",
            "loss: 0.492920  [39168/40000]\n",
            "Val loss: 0.494371 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.515933  [  768/40000]\n",
            "loss: 0.511095  [ 8448/40000]\n",
            "loss: 0.509260  [16128/40000]\n",
            "loss: 0.485241  [23808/40000]\n",
            "loss: 0.500898  [31488/40000]\n",
            "loss: 0.492748  [39168/40000]\n",
            "Val loss: 0.492812 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.514345  [  768/40000]\n",
            "loss: 0.510668  [ 8448/40000]\n",
            "loss: 0.508107  [16128/40000]\n",
            "loss: 0.485009  [23808/40000]\n",
            "loss: 0.500317  [31488/40000]\n",
            "loss: 0.491684  [39168/40000]\n",
            "Val loss: 0.491867 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.513356  [  768/40000]\n",
            "loss: 0.509470  [ 8448/40000]\n",
            "loss: 0.509250  [16128/40000]\n",
            "loss: 0.484673  [23808/40000]\n",
            "loss: 0.500347  [31488/40000]\n",
            "loss: 0.490777  [39168/40000]\n",
            "Val loss: 0.491875 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.513644  [  768/40000]\n",
            "loss: 0.509559  [ 8448/40000]\n",
            "loss: 0.507050  [16128/40000]\n",
            "loss: 0.483228  [23808/40000]\n",
            "loss: 0.499550  [31488/40000]\n",
            "loss: 0.490559  [39168/40000]\n",
            "Val loss: 0.491446 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.513349  [  768/40000]\n",
            "loss: 0.508056  [ 8448/40000]\n",
            "loss: 0.506776  [16128/40000]\n",
            "loss: 0.483364  [23808/40000]\n",
            "loss: 0.498652  [31488/40000]\n",
            "loss: 0.490018  [39168/40000]\n",
            "Val loss: 0.489853 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.511739  [  768/40000]\n",
            "loss: 0.508220  [ 8448/40000]\n",
            "loss: 0.505278  [16128/40000]\n",
            "loss: 0.482586  [23808/40000]\n",
            "loss: 0.497873  [31488/40000]\n",
            "loss: 0.489118  [39168/40000]\n",
            "Val loss: 0.490329 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.511967  [  768/40000]\n",
            "loss: 0.507397  [ 8448/40000]\n",
            "loss: 0.505281  [16128/40000]\n",
            "loss: 0.481972  [23808/40000]\n",
            "loss: 0.497247  [31488/40000]\n",
            "loss: 0.488988  [39168/40000]\n",
            "Val loss: 0.492360 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.514209  [  768/40000]\n",
            "loss: 0.507219  [ 8448/40000]\n",
            "loss: 0.505356  [16128/40000]\n",
            "loss: 0.481148  [23808/40000]\n",
            "loss: 0.496994  [31488/40000]\n",
            "loss: 0.488295  [39168/40000]\n",
            "Val loss: 0.488526 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.510976  [  768/40000]\n",
            "loss: 0.506776  [ 8448/40000]\n",
            "loss: 0.504520  [16128/40000]\n",
            "loss: 0.481704  [23808/40000]\n",
            "loss: 0.496912  [31488/40000]\n",
            "loss: 0.488254  [39168/40000]\n",
            "Val loss: 0.488515 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.511511  [  768/40000]\n",
            "loss: 0.505394  [ 8448/40000]\n",
            "loss: 0.503163  [16128/40000]\n",
            "loss: 0.479612  [23808/40000]\n",
            "loss: 0.495293  [31488/40000]\n",
            "loss: 0.486429  [39168/40000]\n",
            "Val loss: 0.486323 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.509068  [  768/40000]\n",
            "loss: 0.504005  [ 8448/40000]\n",
            "loss: 0.502561  [16128/40000]\n",
            "loss: 0.479459  [23808/40000]\n",
            "loss: 0.494522  [31488/40000]\n",
            "loss: 0.486496  [39168/40000]\n",
            "Val loss: 0.485747 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.508304  [  768/40000]\n",
            "loss: 0.504525  [ 8448/40000]\n",
            "loss: 0.502138  [16128/40000]\n",
            "loss: 0.479825  [23808/40000]\n",
            "loss: 0.494559  [31488/40000]\n",
            "loss: 0.486175  [39168/40000]\n",
            "Val loss: 0.485657 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.508169  [  768/40000]\n",
            "loss: 0.503477  [ 8448/40000]\n",
            "loss: 0.502001  [16128/40000]\n",
            "loss: 0.477867  [23808/40000]\n",
            "loss: 0.493972  [31488/40000]\n",
            "loss: 0.485912  [39168/40000]\n",
            "Val loss: 0.485874 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.509326  [  768/40000]\n",
            "loss: 0.503549  [ 8448/40000]\n",
            "loss: 0.501736  [16128/40000]\n",
            "loss: 0.479912  [23808/40000]\n",
            "loss: 0.494086  [31488/40000]\n",
            "loss: 0.484958  [39168/40000]\n",
            "Val loss: 0.483404 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.507648  [  768/40000]\n",
            "loss: 0.502798  [ 8448/40000]\n",
            "loss: 0.500223  [16128/40000]\n",
            "loss: 0.477290  [23808/40000]\n",
            "loss: 0.492549  [31488/40000]\n",
            "loss: 0.483991  [39168/40000]\n",
            "Val loss: 0.484157 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.508272  [  768/40000]\n",
            "loss: 0.501592  [ 8448/40000]\n",
            "loss: 0.499705  [16128/40000]\n",
            "loss: 0.477415  [23808/40000]\n",
            "loss: 0.492059  [31488/40000]\n",
            "loss: 0.483575  [39168/40000]\n",
            "Val loss: 0.482670 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.506652  [  768/40000]\n",
            "loss: 0.500508  [ 8448/40000]\n",
            "loss: 0.499013  [16128/40000]\n",
            "loss: 0.476202  [23808/40000]\n",
            "loss: 0.492509  [31488/40000]\n",
            "loss: 0.483163  [39168/40000]\n",
            "Val loss: 0.480880 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.505441  [  768/40000]\n",
            "loss: 0.500467  [ 8448/40000]\n",
            "loss: 0.498305  [16128/40000]\n",
            "loss: 0.475164  [23808/40000]\n",
            "loss: 0.490173  [31488/40000]\n",
            "loss: 0.482370  [39168/40000]\n",
            "Val loss: 0.482395 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.505230  [  768/40000]\n",
            "loss: 0.499265  [ 8448/40000]\n",
            "loss: 0.497593  [16128/40000]\n",
            "loss: 0.475982  [23808/40000]\n",
            "loss: 0.489974  [31488/40000]\n",
            "loss: 0.482846  [39168/40000]\n",
            "Val loss: 0.479904 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.504434  [  768/40000]\n",
            "loss: 0.499762  [ 8448/40000]\n",
            "loss: 0.497262  [16128/40000]\n",
            "loss: 0.474566  [23808/40000]\n",
            "loss: 0.488955  [31488/40000]\n",
            "loss: 0.481290  [39168/40000]\n",
            "Val loss: 0.480446 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.504212  [  768/40000]\n",
            "loss: 0.497956  [ 8448/40000]\n",
            "loss: 0.496304  [16128/40000]\n",
            "loss: 0.472378  [23808/40000]\n",
            "loss: 0.488104  [31488/40000]\n",
            "loss: 0.479752  [39168/40000]\n",
            "Val loss: 0.477544 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.502272  [  768/40000]\n",
            "loss: 0.497234  [ 8448/40000]\n",
            "loss: 0.495651  [16128/40000]\n",
            "loss: 0.472044  [23808/40000]\n",
            "loss: 0.487070  [31488/40000]\n",
            "loss: 0.478968  [39168/40000]\n",
            "Val loss: 0.476746 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.502660  [  768/40000]\n",
            "loss: 0.496503  [ 8448/40000]\n",
            "loss: 0.494438  [16128/40000]\n",
            "loss: 0.471490  [23808/40000]\n",
            "loss: 0.486422  [31488/40000]\n",
            "loss: 0.479115  [39168/40000]\n",
            "Val loss: 0.476436 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.500331  [  768/40000]\n",
            "loss: 0.495901  [ 8448/40000]\n",
            "loss: 0.493836  [16128/40000]\n",
            "loss: 0.470866  [23808/40000]\n",
            "loss: 0.485986  [31488/40000]\n",
            "loss: 0.478192  [39168/40000]\n",
            "Val loss: 0.475123 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.500119  [  768/40000]\n",
            "loss: 0.494886  [ 8448/40000]\n",
            "loss: 0.492814  [16128/40000]\n",
            "loss: 0.470201  [23808/40000]\n",
            "loss: 0.484527  [31488/40000]\n",
            "loss: 0.477348  [39168/40000]\n",
            "Val loss: 0.477136 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.502190  [  768/40000]\n",
            "loss: 0.494805  [ 8448/40000]\n",
            "loss: 0.492831  [16128/40000]\n",
            "loss: 0.469794  [23808/40000]\n",
            "loss: 0.483865  [31488/40000]\n",
            "loss: 0.476816  [39168/40000]\n",
            "Val loss: 0.474288 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.498906  [  768/40000]\n",
            "loss: 0.496068  [ 8448/40000]\n",
            "loss: 0.492993  [16128/40000]\n",
            "loss: 0.468848  [23808/40000]\n",
            "loss: 0.484073  [31488/40000]\n",
            "loss: 0.476016  [39168/40000]\n",
            "Val loss: 0.473791 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.499145  [  768/40000]\n",
            "loss: 0.493709  [ 8448/40000]\n",
            "loss: 0.491517  [16128/40000]\n",
            "loss: 0.468909  [23808/40000]\n",
            "loss: 0.483533  [31488/40000]\n",
            "loss: 0.476133  [39168/40000]\n",
            "Val loss: 0.473952 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.499808  [  768/40000]\n",
            "loss: 0.492709  [ 8448/40000]\n",
            "loss: 0.491596  [16128/40000]\n",
            "loss: 0.468080  [23808/40000]\n",
            "loss: 0.482823  [31488/40000]\n",
            "loss: 0.475541  [39168/40000]\n",
            "Val loss: 0.471429 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.497869  [  768/40000]\n",
            "loss: 0.493864  [ 8448/40000]\n",
            "loss: 0.490301  [16128/40000]\n",
            "loss: 0.468732  [23808/40000]\n",
            "loss: 0.482527  [31488/40000]\n",
            "loss: 0.474367  [39168/40000]\n",
            "Val loss: 0.471482 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.497265  [  768/40000]\n",
            "loss: 0.492085  [ 8448/40000]\n",
            "loss: 0.490810  [16128/40000]\n",
            "loss: 0.469099  [23808/40000]\n",
            "loss: 0.482925  [31488/40000]\n",
            "loss: 0.475109  [39168/40000]\n",
            "Val loss: 0.471166 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.496615  [  768/40000]\n",
            "loss: 0.492463  [ 8448/40000]\n",
            "loss: 0.490153  [16128/40000]\n",
            "loss: 0.467202  [23808/40000]\n",
            "loss: 0.482024  [31488/40000]\n",
            "loss: 0.474901  [39168/40000]\n",
            "Val loss: 0.470772 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.496356  [  768/40000]\n",
            "loss: 0.491370  [ 8448/40000]\n",
            "loss: 0.488996  [16128/40000]\n",
            "loss: 0.467043  [23808/40000]\n",
            "loss: 0.481501  [31488/40000]\n",
            "loss: 0.473830  [39168/40000]\n",
            "Val loss: 0.469973 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.496214  [  768/40000]\n",
            "loss: 0.491200  [ 8448/40000]\n",
            "loss: 0.489159  [16128/40000]\n",
            "loss: 0.466084  [23808/40000]\n",
            "loss: 0.480419  [31488/40000]\n",
            "loss: 0.473672  [39168/40000]\n",
            "Val loss: 0.471180 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.496346  [  768/40000]\n",
            "loss: 0.490784  [ 8448/40000]\n",
            "loss: 0.489057  [16128/40000]\n",
            "loss: 0.466122  [23808/40000]\n",
            "loss: 0.480351  [31488/40000]\n",
            "loss: 0.473211  [39168/40000]\n",
            "Val loss: 0.470258 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.496504  [  768/40000]\n",
            "loss: 0.491196  [ 8448/40000]\n",
            "loss: 0.489119  [16128/40000]\n",
            "loss: 0.466305  [23808/40000]\n",
            "loss: 0.480769  [31488/40000]\n",
            "loss: 0.473778  [39168/40000]\n",
            "Val loss: 0.469849 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.495516  [  768/40000]\n",
            "loss: 0.489940  [ 8448/40000]\n",
            "loss: 0.488486  [16128/40000]\n",
            "loss: 0.465872  [23808/40000]\n",
            "loss: 0.480168  [31488/40000]\n",
            "loss: 0.472728  [39168/40000]\n",
            "Val loss: 0.470436 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.494960  [  768/40000]\n",
            "loss: 0.491007  [ 8448/40000]\n",
            "loss: 0.487860  [16128/40000]\n",
            "loss: 0.464852  [23808/40000]\n",
            "loss: 0.481111  [31488/40000]\n",
            "loss: 0.473216  [39168/40000]\n",
            "Val loss: 0.468405 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.493703  [  768/40000]\n",
            "loss: 0.489134  [ 8448/40000]\n",
            "loss: 0.488085  [16128/40000]\n",
            "loss: 0.465403  [23808/40000]\n",
            "loss: 0.480263  [31488/40000]\n",
            "loss: 0.473202  [39168/40000]\n",
            "Val loss: 0.469841 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.494801  [  768/40000]\n",
            "loss: 0.489856  [ 8448/40000]\n",
            "loss: 0.487949  [16128/40000]\n",
            "loss: 0.465646  [23808/40000]\n",
            "loss: 0.480523  [31488/40000]\n",
            "loss: 0.471976  [39168/40000]\n",
            "Val loss: 0.470141 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.495270  [  768/40000]\n",
            "loss: 0.488826  [ 8448/40000]\n",
            "loss: 0.487874  [16128/40000]\n",
            "loss: 0.464994  [23808/40000]\n",
            "loss: 0.480372  [31488/40000]\n",
            "loss: 0.472215  [39168/40000]\n",
            "Val loss: 0.471528 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.497545  [  768/40000]\n",
            "loss: 0.488302  [ 8448/40000]\n",
            "loss: 0.487586  [16128/40000]\n",
            "loss: 0.464509  [23808/40000]\n",
            "loss: 0.479683  [31488/40000]\n",
            "loss: 0.471774  [39168/40000]\n",
            "Val loss: 0.468484 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.494542  [  768/40000]\n",
            "loss: 0.489603  [ 8448/40000]\n",
            "loss: 0.488547  [16128/40000]\n",
            "loss: 0.465126  [23808/40000]\n",
            "loss: 0.480544  [31488/40000]\n",
            "loss: 0.472074  [39168/40000]\n",
            "Val loss: 0.469338 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.494714  [  768/40000]\n",
            "loss: 0.488837  [ 8448/40000]\n",
            "loss: 0.487270  [16128/40000]\n",
            "loss: 0.464346  [23808/40000]\n",
            "loss: 0.478635  [31488/40000]\n",
            "loss: 0.471546  [39168/40000]\n",
            "Val loss: 0.467765 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.493601  [  768/40000]\n",
            "loss: 0.488883  [ 8448/40000]\n",
            "loss: 0.487376  [16128/40000]\n",
            "loss: 0.464391  [23808/40000]\n",
            "loss: 0.478764  [31488/40000]\n",
            "loss: 0.471439  [39168/40000]\n",
            "Val loss: 0.468486 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.494002  [  768/40000]\n",
            "loss: 0.487387  [ 8448/40000]\n",
            "loss: 0.487232  [16128/40000]\n",
            "loss: 0.463329  [23808/40000]\n",
            "loss: 0.478760  [31488/40000]\n",
            "loss: 0.471517  [39168/40000]\n",
            "Val loss: 0.468789 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.494930  [  768/40000]\n",
            "loss: 0.488044  [ 8448/40000]\n",
            "loss: 0.486568  [16128/40000]\n",
            "loss: 0.463859  [23808/40000]\n",
            "loss: 0.478920  [31488/40000]\n",
            "loss: 0.471589  [39168/40000]\n",
            "Val loss: 0.469260 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.495164  [  768/40000]\n",
            "loss: 0.487779  [ 8448/40000]\n",
            "loss: 0.488488  [16128/40000]\n",
            "loss: 0.464494  [23808/40000]\n",
            "loss: 0.478497  [31488/40000]\n",
            "loss: 0.470291  [39168/40000]\n",
            "Val loss: 0.468145 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.493704  [  768/40000]\n",
            "loss: 0.487685  [ 8448/40000]\n",
            "loss: 0.486130  [16128/40000]\n",
            "loss: 0.464117  [23808/40000]\n",
            "loss: 0.479126  [31488/40000]\n",
            "loss: 0.471134  [39168/40000]\n",
            "Val loss: 0.467652 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.492433  [  768/40000]\n",
            "loss: 0.488476  [ 8448/40000]\n",
            "loss: 0.487655  [16128/40000]\n",
            "loss: 0.464246  [23808/40000]\n",
            "loss: 0.478643  [31488/40000]\n",
            "loss: 0.470842  [39168/40000]\n",
            "Val loss: 0.467841 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.492555  [  768/40000]\n",
            "loss: 0.487650  [ 8448/40000]\n",
            "loss: 0.486259  [16128/40000]\n",
            "loss: 0.463260  [23808/40000]\n",
            "loss: 0.478358  [31488/40000]\n",
            "loss: 0.469690  [39168/40000]\n",
            "Val loss: 0.467197 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.492527  [  768/40000]\n",
            "loss: 0.488423  [ 8448/40000]\n",
            "loss: 0.486357  [16128/40000]\n",
            "loss: 0.464083  [23808/40000]\n",
            "loss: 0.477879  [31488/40000]\n",
            "loss: 0.471157  [39168/40000]\n",
            "Val loss: 0.467327 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.492540  [  768/40000]\n",
            "loss: 0.487288  [ 8448/40000]\n",
            "loss: 0.486837  [16128/40000]\n",
            "loss: 0.462820  [23808/40000]\n",
            "loss: 0.478199  [31488/40000]\n",
            "loss: 0.471108  [39168/40000]\n",
            "Val loss: 0.466770 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.492164  [  768/40000]\n",
            "loss: 0.487007  [ 8448/40000]\n",
            "loss: 0.486451  [16128/40000]\n",
            "loss: 0.462999  [23808/40000]\n",
            "loss: 0.477687  [31488/40000]\n",
            "loss: 0.471195  [39168/40000]\n",
            "Val loss: 0.467547 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.493452  [  768/40000]\n",
            "loss: 0.486817  [ 8448/40000]\n",
            "loss: 0.485829  [16128/40000]\n",
            "loss: 0.462498  [23808/40000]\n",
            "loss: 0.477493  [31488/40000]\n",
            "loss: 0.470365  [39168/40000]\n",
            "Val loss: 0.467198 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.492806  [  768/40000]\n",
            "loss: 0.487686  [ 8448/40000]\n",
            "loss: 0.485266  [16128/40000]\n",
            "loss: 0.462546  [23808/40000]\n",
            "loss: 0.477826  [31488/40000]\n",
            "loss: 0.469688  [39168/40000]\n",
            "Val loss: 0.467288 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cur_val_loss = np.inf\n",
        "cur_epoch = 1\n",
        "cur_state_dict = None\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    if test(val_dataloader, model, loss_fn) < cur_val_loss:\n",
        "        cur_epoch = t + 1\n",
        "        cur_state_dict = model.module.state_dict()\n",
        "        torch.save({\n",
        "            'epoch': cur_epoch,\n",
        "            'model_state_dict': cur_state_dict,\n",
        "            }, 'transformer_model.pt')\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxXsAbLocuce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxXsAbLocuce",
        "outputId": "49020e2b-3346-49f3-e41b-84bfb73f30a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (_read_in): Linear(in_features=13, out_features=32, bias=True)\n",
              "  (_backbone): GPT2Model(\n",
              "    (wte): Embedding(50257, 32)\n",
              "    (wpe): Embedding(201, 32)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x GPT2Block(\n",
              "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (_read_out): Linear(in_features=32, out_features=10, bias=True)\n",
              "  (_flatten): Flatten(start_dim=0, end_dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model = TransformerModel(n_states=10)\n",
        "model.to(device)\n",
        "dict = torch.load('./transformer_model.pt')['model_state_dict']\n",
        "model.load_state_dict(dict, strict=False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VnPOvAbHdKnK",
      "metadata": {
        "id": "VnPOvAbHdKnK"
      },
      "outputs": [],
      "source": [
        "def generate():\n",
        "    a, r, mus = generate_in_context_dataset()\n",
        "\n",
        "    a_one_hot = np.zeros((n, A))\n",
        "    a_one_hot[np.arange(n), a] = 1\n",
        "\n",
        "    X = np.zeros((n, A + 3), np.float32)\n",
        "    X[:, 0] = 1\n",
        "    X[:, 1:A + 1] = a_one_hot\n",
        "    X[:, -2] = 1\n",
        "    X[:, -1] = r\n",
        "\n",
        "    return X, mus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZskWZFEOc207",
      "metadata": {
        "id": "ZskWZFEOc207"
      },
      "outputs": [],
      "source": [
        "# reg = np.empty((200, n - 1))\n",
        "\n",
        "# for i in range(200):\n",
        "#     X, mu, bandit = generate()\n",
        "#     X = torch.unsqueeze(torch.from_numpy(X).to(device), 0)\n",
        "#     with torch.no_grad():\n",
        "#         prediction = model.forward(X).cpu().numpy()\n",
        "#     reg[i] = np.max(mu) - mu[prediction.argmax(1)]\n",
        "# reg = reg.mean(0)\n",
        "\n",
        "reg = np.empty((200, n - 1))\n",
        "\n",
        "for trial in range(200):\n",
        "  X, mu = generate()\n",
        "  X = torch.unsqueeze(torch.from_numpy(X).to(device), 0)\n",
        "  with torch.no_grad():\n",
        "    prediction = model.forward(X).cpu().numpy()\n",
        "  reg[trial] = np.max(mu) - mu[prediction.argmax(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xr_wwwctdAkq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "xr_wwwctdAkq",
        "outputId": "2ab7b46b-ebb5-4dcd-ab78-a08467179c49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.arange(n - 1), reg.mean(0))\n",
        "plt.savefig('offline.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hGyfVzTYVNcZ",
      "metadata": {
        "id": "hGyfVzTYVNcZ"
      },
      "outputs": [],
      "source": [
        "reg_on = np.empty((200, n - 1))\n",
        "for i in range(200):\n",
        "    #mu = np.random.rand(A)\n",
        "    theta = np.random.normal(0, np.sqrt(1/d), size=d)\n",
        "    mu = np.dot(phi, theta)\n",
        "    X = torch.zeros((1, n + 1, A + 3), device=device)\n",
        "    X[0, :, 0] = 1\n",
        "    X[0, :, -2] = 1\n",
        "    for j in range(n - 1):\n",
        "        with torch.no_grad():\n",
        "            prediction = torch.softmax(model(X)[j], 0).cpu().numpy()\n",
        "        a = np.random.choice(A, p=prediction)\n",
        "        reg_on[i][j] = np.max(mu) - mu[a]\n",
        "\n",
        "        if j == n:\n",
        "            break\n",
        "\n",
        "        X[0, j + 1, a + 1] = 1\n",
        "        X[0, j + 1, -1] = mu[a] #np.random.normal(mu[a], 0.3)\n",
        "reg_on = reg_on.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l0eDEl7fVfXI",
      "metadata": {
        "id": "l0eDEl7fVfXI"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(n - 1), reg.cumsum())\n",
        "plt.savefig('online.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}