{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce21e60-73f2-421d-b8d5-c8a7023b20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Model, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdba263-b726-4316-80b5-2be5e2fb6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d945d95-155d-4bed-b879-5af6f0d32f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 5 # number of actions\n",
    "N = 80000 # number of offline samples\n",
    "n = 500\n",
    "eps = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae1187e-a08c-4632-b28a-341cba54f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_B(A=5, N=80000, n=500):\n",
    "    dsets, v_dsets, actions, coefficients = [], [], [], []\n",
    "    for i in range(N):\n",
    "        p_1 = np.random.dirichlet(np.ones(A))\n",
    "        p_2 = np.zeros(A)\n",
    "        idx = np.random.choice(A)\n",
    "        p_2[idx] = 1\n",
    "        w = (np.random.choice(10) + 1) / 10\n",
    "        p = (1 - w) * p_1 + w * p_2\n",
    "        \n",
    "        a = np.random.choice(A, n, p=p)\n",
    "        actions.append(a)\n",
    "        mu = np.random.rand(A)\n",
    "        coefficients.append(np.exp(mu[a] - np.dot(p, mu)))\n",
    "        r = np.random.normal(mu[a], 0.3)\n",
    "        \n",
    "        a_one_hot = np.zeros((n, A))\n",
    "        a_one_hot[np.arange(n), a] = 1\n",
    "\n",
    "        X = np.zeros((n, A + 3), np.float32)\n",
    "        X[:, 0] = 1\n",
    "        X[:, 1:A + 1] = a_one_hot\n",
    "        X[:, -2] = 1\n",
    "        X[:, -1] = r\n",
    "        dsets.append(X)\n",
    "    return dsets, actions, coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8224dd6d-b47a-4590-b45f-533496b9368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditDataset(Dataset):\n",
    "    def __init__(self, dsets, actions, coefficients):\n",
    "        self.dsets = dsets\n",
    "        self.actions = actions\n",
    "        self.coefficients = coefficients\n",
    "        \n",
    "        self.first = np.zeros((1, A + 3), dtype=np.float32)\n",
    "        self.first[0, 0] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dsets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = np.random.permutation(n)\n",
    "        sample_ds = self.dsets[idx]\n",
    "        sample_ds = np.concatenate((self.first, sample_ds[p]))\n",
    "        sample_actions = self.actions[idx][p]\n",
    "        sample_coefficients = self.coefficients[idx][p]\n",
    "        \n",
    "        return sample_ds, sample_actions, sample_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de84ccd0-4156-4f20-9b53-1fc8be5f7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_states, n_positions=501, n_embd=32, n_layer=4, n_head=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        configuration = GPT2Config(\n",
    "            n_positions=n_positions,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "        )\n",
    "        self.name = f\"gpt2_embd={n_embd}_layer={n_layer}_head={n_head}\"\n",
    "\n",
    "        self.n_positions = n_positions\n",
    "        self.n_dims = n_states\n",
    "        self._read_in = nn.Linear(n_states + 3, n_embd)\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "        self._read_out = nn.Linear(n_embd, 5)\n",
    "        self._flatten = nn.Flatten(0, 1)\n",
    "\n",
    "        for w in self._backbone.wpe.parameters():\n",
    "            w.data.fill_(0)\n",
    "        self._backbone.wpe.weight.requires_grad=False\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeds = self._read_in(X)\n",
    "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
    "        logit = self._read_out(output)[:, :-1]\n",
    "        logit = self._flatten(logit)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6c5d62-3b19-48cf-bc06-804d504b1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, a, c):\n",
    "    ce_loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    return torch.mean(ce_loss_fn(pred, a) * c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8289d6-2481-4a96-8ef6-7255d573d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, a, c) in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        a = a.flatten().to(device)\n",
    "        c = c.flatten().to(device)\n",
    "    \n",
    "        loss = loss_fn(pred, a, c)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7873169-e185-4254-9248-99591294a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, a, c in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            a = a.flatten().to(device)\n",
    "            c = c.flatten().to(device)\n",
    "        \n",
    "            loss = loss_fn(pred, a, c)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Val loss: {val_loss:>8f} \\n\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e4c60a-d79c-40ba-85e5-a733a4a36a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_train, actions_train, coefficients_train = generate_B(N=N)\n",
    "dsets_val, actions_val, coefficients_val = generate_B(N=N//4)\n",
    "\n",
    "data_train = BanditDataset(dsets_train, actions_train, coefficients_train)\n",
    "data_val = BanditDataset(dsets_val, actions_val, coefficients_val)\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size=768)\n",
    "val_dataloader = DataLoader(data_val, batch_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74e74bb-0651-464e-8942-d5a50719c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(n_states=5)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259356c4-b704-45b9-aab2-1d789009d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([param for param in model.parameters() if param.requires_grad == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051de06-dde7-4825-b451-3cf9822fd774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.726258  [  768/80000]\n",
      "loss: 1.416939  [ 8448/80000]\n",
      "loss: 1.229136  [16128/80000]\n",
      "loss: 1.101623  [23808/80000]\n",
      "loss: 1.023265  [31488/80000]\n",
      "loss: 0.958785  [39168/80000]\n",
      "loss: 0.993562  [46848/80000]\n",
      "loss: 0.975385  [54528/80000]\n",
      "loss: 0.958699  [62208/80000]\n",
      "loss: 0.936249  [69888/80000]\n",
      "loss: 0.978076  [77568/80000]\n",
      "Val loss: 0.935875 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.961182  [  768/80000]\n",
      "loss: 0.937805  [ 8448/80000]\n",
      "loss: 0.929597  [16128/80000]\n",
      "loss: 0.957975  [23808/80000]\n",
      "loss: 0.942794  [31488/80000]\n",
      "loss: 0.905985  [39168/80000]\n",
      "loss: 0.951351  [46848/80000]\n",
      "loss: 0.940382  [54528/80000]\n",
      "loss: 0.933921  [62208/80000]\n",
      "loss: 0.918316  [69888/80000]\n",
      "loss: 0.962561  [77568/80000]\n",
      "Val loss: 0.925179 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.947169  [  768/80000]\n",
      "loss: 0.926705  [ 8448/80000]\n",
      "loss: 0.919874  [16128/80000]\n",
      "loss: 0.948416  [23808/80000]\n",
      "loss: 0.934673  [31488/80000]\n",
      "loss: 0.898814  [39168/80000]\n",
      "loss: 0.943153  [46848/80000]\n",
      "loss: 0.932815  [54528/80000]\n",
      "loss: 0.927204  [62208/80000]\n",
      "loss: 0.911818  [69888/80000]\n",
      "loss: 0.957025  [77568/80000]\n",
      "Val loss: 0.920066 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.940926  [  768/80000]\n",
      "loss: 0.921275  [ 8448/80000]\n",
      "loss: 0.914230  [16128/80000]\n",
      "loss: 0.943932  [23808/80000]\n",
      "loss: 0.930465  [31488/80000]\n",
      "loss: 0.894544  [39168/80000]\n",
      "loss: 0.939738  [46848/80000]\n",
      "loss: 0.930170  [54528/80000]\n",
      "loss: 0.924180  [62208/80000]\n",
      "loss: 0.909402  [69888/80000]\n",
      "loss: 0.954450  [77568/80000]\n",
      "Val loss: 0.918037 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.938241  [  768/80000]\n",
      "loss: 0.919402  [ 8448/80000]\n",
      "loss: 0.912766  [16128/80000]\n",
      "loss: 0.942180  [23808/80000]\n",
      "loss: 0.928692  [31488/80000]\n",
      "loss: 0.893117  [39168/80000]\n",
      "loss: 0.938134  [46848/80000]\n",
      "loss: 0.928567  [54528/80000]\n",
      "loss: 0.923113  [62208/80000]\n",
      "loss: 0.907916  [69888/80000]\n",
      "loss: 0.953570  [77568/80000]\n",
      "Val loss: 0.917834 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.937782  [  768/80000]\n",
      "loss: 0.918794  [ 8448/80000]\n",
      "loss: 0.911858  [16128/80000]\n",
      "loss: 0.941245  [23808/80000]\n",
      "loss: 0.928152  [31488/80000]\n",
      "loss: 0.892563  [39168/80000]\n",
      "loss: 0.937365  [46848/80000]\n",
      "loss: 0.927750  [54528/80000]\n",
      "loss: 0.922628  [62208/80000]\n",
      "loss: 0.907249  [69888/80000]\n",
      "loss: 0.952551  [77568/80000]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cur_val_loss = np.inf\n",
    "cur_epoch = 1\n",
    "cur_state_dict = None\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    if test(val_dataloader, model, loss_fn) < cur_val_loss:\n",
    "        cur_epoch = t + 1\n",
    "        cur_state_dict = model.module.state_dict()\n",
    "        torch.save({\n",
    "            'epoch': cur_epoch,\n",
    "            'model_state_dict': cur_state_dict,\n",
    "            }, 'transformer_model.pt')\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
